{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 2: Latent semantic indexing\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** *Y*\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* *Kristian Aurlien*\n",
    "* *Mateusz Paluchowski*\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 2 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Donâ€™t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.sparse.linalg import svds\n",
    "from utils import load_json, load_pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.4: Latent semantic indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of terms (n) = 10875\n",
      "Number of courses (m) = 854\n"
     ]
    }
   ],
   "source": [
    "# n x m matrix where n is number of terms and m is number of documents\n",
    "X = load_pkl('tfidx_matrix.pkl')\n",
    "terms = load_pkl('terms.pkl')\n",
    "courses = load_pkl('courses.pkl')\n",
    "\n",
    "n, m = X.shape\n",
    "print('Number of terms (n) =', n)\n",
    "print('Number of courses (m) =', m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "U, S, V_t = svds(X, k=300, which='LM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U: (10875, 300)\n",
      "S: (300,)\n",
      "V^T: (300, 854)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "I'm in love with the shape of U\n",
    "We push and pull like a magnet do\n",
    "Although my heart is falling too\n",
    "I'm in love with your body\n",
    "'''\n",
    "\n",
    "print('U:', U.shape)\n",
    "print('S:', S.shape)\n",
    "print('V^T:', V_t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. Describe the rows and columns of U and V , and the values of S.\n",
    "#### $U$: Term-concept mapping\n",
    "\n",
    "The $n$ rows of the $U$-matrix given by the SVD, gives us a mapping from term to concept. Each row is the mapping for one term, and each value $v_i$ in that row shows how strongly that term relates to concept $c_i$\n",
    "\n",
    "#### $V^T$: Course-concept mapping\n",
    "Similarly, the $m$ columns of the $V^T$ matrix shows how strongly each course corresponds to each concept.\n",
    "\n",
    "#### $S$: Concept-\"strength\"\n",
    "The singular valies of $S$ shows how \"strong\" the concept is - the bigger the value is, the \"stronger\" the concept.\n",
    "\n",
    "\n",
    "    2. Print the top-20 eigenvalues of X.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 349.98468286,  223.52636182,  211.60463826,  204.95465982,\n",
       "        192.72277657,  191.73736178,  188.90578214,  186.88000684,\n",
       "        182.26580104,  177.04396616,  172.45494585,  171.3265112 ,\n",
       "        168.98098613,  164.52156741,  161.13242853,  160.75631113,\n",
       "        158.58393064,  157.0648853 ,  155.26142247,  153.10302775])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top20_singular = S[::-1][:20]\n",
    "top20_singular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[122489.27823503187,\n",
       " 49964.034429859865,\n",
       " 44776.522932370121,\n",
       " 42006.412581026918,\n",
       " 37142.068610248956,\n",
       " 36763.215903532655,\n",
       " 35685.394527661541,\n",
       " 34924.136956224007,\n",
       " 33220.822229130361,\n",
       " 31344.56595271128,\n",
       " 29740.708346997264,\n",
       " 29352.773441369507,\n",
       " 28554.573672492083,\n",
       " 27067.346143503579,\n",
       " 25963.659525275787,\n",
       " 25842.591568733384,\n",
       " 25148.863057568571,\n",
       " 24669.378195278958,\n",
       " 24106.1093074325,\n",
       " 23440.537106715161]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top20_eigenvalues = [x*x for x in top20_singular]\n",
    "top20_eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.5: Topic extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = np.diag(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10875,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U[:,-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept 1 :\n",
      "['energi', 'electron', 'materi', 'data', 'project', 'optic', 'process', 'model', 'design', 'system']\n",
      "Concept 2 :\n",
      "['sensor', 'materi', 'print', 'imag', 'light', 'devic', 'microscopi', 'laser', 'electron', 'optic']\n",
      "Concept 3 :\n",
      "['excurs', 'form', 'report', 'design', 'week', 'urban', 'studio', 'project', 'architectur', 'wast']\n",
      "Concept 4 :\n",
      "['algorithm', 'robot', 'system', 'speech', 'design', 'data', 'urban', 'digit', 'studio', 'architectur']\n",
      "Concept 5 :\n",
      "['waveguid', 'citi', 'light', 'urban', 'studio', 'imag', 'architectur', 'laser', 'wast', 'optic']\n",
      "Concept 6 :\n",
      "['power', 'wast', 'studio', 'electron', 'devic', 'circuit', 'design', 'steel', 'print', 'architectur']\n",
      "Concept 7 :\n",
      "['common', 'polici', 'chemic', 'print', 'electron', 'protein', 'architectur', 'cell', 'energi', 'risk']\n",
      "Concept 8 :\n",
      "['algorithm', 'voic', 'model', 'code', 'robot', 'process', 'wast', 'signal', 'recognit', 'speech']\n",
      "Concept 9 :\n",
      "['electron', 'host', 'wearabl', 'circuit', 'report', 'lab', 'sensor', 'devic', 'print', 'robot']\n",
      "Concept 10 :\n",
      "['synthesi', 'electron', 'steel', 'voic', 'risk', 'biolog', 'recognit', 'cell', 'protein', 'speech']\n"
     ]
    }
   ],
   "source": [
    "# combination of terms\n",
    "for concept in range(-1,-11,-1):\n",
    "    words = [terms[t] for t in np.argsort(U[:,concept])[-10:]]\n",
    "    print('Concept', -concept,':')\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept 1 :\n",
      "['AR-402(y)', 'MICRO-505', 'HUM-370', 'FIN-404', 'AR-401(y)', 'MSE-803', 'EE-730', 'FIN-402', 'ENV-500', 'ENG-421']\n",
      "Concept 2 :\n",
      "['MICRO-562', 'BIOENG-445', 'MICRO-534', 'CH-448', 'MICRO-421', 'MICRO-504', 'MICRO-424', 'MICRO-618', 'MICRO-505', 'MSE-803']\n",
      "Concept 3 :\n",
      "['AR-401(b)', 'AR-402(c)', 'AR-401(c)', 'BIO-504', 'BIO-505', 'BIO-506', 'BIO-507', 'AR-402(y)', 'AR-401(y)', 'ENV-500']\n",
      "Concept 4 :\n",
      "['AR-401(b)', 'AR-476', 'EE-553', 'MICRO-453', 'EE-432', 'EE-730', 'AR-402(c)', 'AR-401(c)', 'AR-402(y)', 'AR-401(y)']\n",
      "Concept 5 :\n",
      "['MICRO-422', 'MICRO-421', 'AR-402(y)', 'BIOENG-445', 'CH-448', 'AR-401(y)', 'MICRO-424', 'AR-402(c)', 'AR-401(c)', 'ENV-500']\n",
      "Concept 6 :\n",
      "['EE-730', 'ENV-500', 'AR-402(y)', 'MSE-803', 'AR-402(c)', 'AR-401(c)', 'MICRO-618', 'AR-401(y)', 'MICRO-505', 'CIVIL-435']\n",
      "Concept 7 :\n",
      "['Caution, these contents corresponds to the coursebooks of last year', 'AR-401(b)', 'AR-401(y)', 'MICRO-505', 'FIN-402', 'MICRO-618', 'MSE-803', 'HUM-370', 'AR-402(c)', 'AR-401(c)']\n",
      "Concept 8 :\n",
      "['EE-612', 'ME-571', 'MICRO-453', 'EE-432', 'EE-704', 'EE-730', 'EE-719', 'ENV-500', 'EE-554', 'EE-553']\n",
      "Concept 9 :\n",
      "['MSE-803', 'MICRO-534', 'EE-730', 'BIO-505', 'MICRO-505', 'BIO-504', 'MICRO-453', 'MICRO-618', 'BIO-506', 'BIO-507']\n",
      "Concept 10 :\n",
      "['MICRO-618', 'BIO-494', 'CH-312', 'FIN-402', 'MSE-638', 'BIO-447', 'EE-719', 'CIVIL-435', 'EE-554', 'EE-553']\n"
     ]
    }
   ],
   "source": [
    "# combination of documents\n",
    "for concept in range(-1,-11,-1):\n",
    "    words = [courses[t]['courseId'] for t in np.argsort(V_t[concept])[-10:]]\n",
    "    print('Concept', -concept,':')\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: 2. Give a label to each of the concept.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.6: Document similarity search in concept-space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "U, S, V_t = svds(X, k=300, which='LM')\n",
    "s = np.diag(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def term_document_similarity(query):\n",
    "    similarities = np.zeros(len(courses))\n",
    "    for term in query.split(' '):\n",
    "        t_index = terms.index(term)\n",
    "        u_t = U[t_index]\n",
    "        \n",
    "        for ix, d in enumerate(courses):\n",
    "            v_T_d = V_t[:,ix]\n",
    "    \n",
    "            s_v_T_d = np.dot(s, v_T_d)\n",
    "            nominator = np.dot(u_t, s_v_T_d)\n",
    "            denominator = np.linalg.norm(u_t) * np.linalg.norm(s_v_T_d)\n",
    "            sim = nominator / denominator\n",
    "            similarities[ix] += sim\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def LSI_search(terms, no_top=5):\n",
    "    similiarities = term_document_similarity(terms)\n",
    "    top_results = np.argsort(similiarities)[::-1][0:no_top]\n",
    "    for top in top_results:\n",
    "        print('{0}: {1}'.format(courses[top]['name'], top))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computational Social Media: 798\n",
      "Social media: 407\n",
      "Studio MA2 (Escher et GuneWardena): 59\n",
      "Transport phenomena II: 29\n",
      "Human computer interaction: 521\n"
     ]
    }
   ],
   "source": [
    "LSI_search('facebook', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied stochastic processes: 80\n",
      "Applied probability & stochastic processes: 398\n",
      "Markov chains and algorithmic applications: 245\n",
      "Supply chain management: 44\n",
      "Mathematical models in supply chain management: 99\n"
     ]
    }
   ],
   "source": [
    "LSI_search('markov chain', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VSM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###### Query: \"facebook\"\n",
    "\n",
    "'Computational Social Media': 0.17945984867925702,\n",
    "\n",
    "'CCMX Advanced Course - Instrumented Nanoindentation': 0.0,\n",
    "\n",
    "'Electronic properties of solids and superconductivity': 0.0,\n",
    "\n",
    "'Hydrogeophysics': 0.0,\n",
    "\n",
    "'Molecular and cellular biophysic II': 0.0\n",
    "\n",
    "\n",
    "###### Query: \"markov chains\"\n",
    "\n",
    "\n",
    "'Applied probability & stochastic processes', 0.55400769353626178,\n",
    "\n",
    "'Applied stochastic processes', 0.55211833344995098,\n",
    "\n",
    "'Markov chains and algorithmic applications', 0.38168653789318985,\n",
    "\n",
    "'Supply chain management', 0.37852761365218429,\n",
    "\n",
    "'Mathematical models in supply chain management', 0.31162506776787757"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Query: \"facebook\"\n",
    "\n",
    "Computational Social Media: 798\n",
    "\n",
    "Human computer interaction: 521\n",
    "\n",
    "Social media: 407\n",
    "\n",
    "Studio MA2 (Escher et GuneWardena): 59\n",
    "\n",
    "Transport phenomena II: 29\n",
    "\n",
    "###### Query: \"markov chains\"\n",
    "\n",
    "Applied probability & stochastic processes: 398\n",
    "\n",
    "Markov chains and algorithmic applications: 245\n",
    "\n",
    "Mathematical models in supply chain management: 99\n",
    "\n",
    "Applied stochastic processes: 80\n",
    "\n",
    "\n",
    "Supply chain management: 44\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above LSI search performs significantly better especially for queries which contain terms occuring only in few of the documents. That is simply because rare terms can be 'asigned' to more general concept describing documents and thus there is a higher chance of finding relevant information in this concept space, rather than in naive term frequency apporach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.7: Document-document similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simpliest and most efficient way of computing document-document similarity is to compute cosine similarity between given document represented as document-concept V_t and rest of the document-concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_sim(d1, d2):\n",
    "    return np.dot(d1, d2) / (np.linalg.norm(d1) * np.linalg.norm(d2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COM_308_id = next(index for (index, d) in enumerate(courses) if d[\"courseId\"] == \"COM-308\")\n",
    "COM_308_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "COM_308_similarities = np.apply_along_axis(cosine_sim, 0, V_t, V_t[:, COM_308_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_5_COM_308 = np.argsort(COM_308_similarities)[::-1][0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_COM_308 = {}\n",
    "for ind, top in enumerate(top_5_COM_308):\n",
    "    if top != COM_308_id:\n",
    "        top_COM_308[courses[top]['name']] = np.sort(COM_308_similarities)[::-1][ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A Network Tour of Data Science': 0.34372597801585575,\n",
       " 'Data science for business': 0.26147945878886258,\n",
       " 'Distributed information systems': 0.53567488105024297,\n",
       " 'Financial big data': 0.4312145971894219,\n",
       " 'Networks out of control': 0.35117786627061676}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_COM_308"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see recommended classes revolve around big data and data science. Deep dive into the course description only seems to verify that proposed classes are accurate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
